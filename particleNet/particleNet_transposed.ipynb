{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ParticleNet transposed\n",
    "\n",
    "In this notebook we try to implement a decoder for the ParticleNet model in order tu use it in an autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # plotting library\n",
    "import numpy as np # this module is useful to work with numerical arrays\n",
    "import pandas as pd # this module is useful to work with tabular data\n",
    "import random # this module will be used to select random samples from a collection\n",
    "import os # this module will be used just to create directories in the local filesystem\n",
    "from tqdm.notebook import tqdm # this module is useful to plot progress bars\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EdgeConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Edge Convolution Block ###########\n",
    "# The root block of our DNN.\n",
    "# Initialized by:\n",
    "#   - d     the number of features\n",
    "#   - k     number of nearest neighbours to consider in the concolution\n",
    "#   - C     a list-like or an int with the number of neurons of the three linear layers\n",
    "#   - aggr  the aggregation function, must be symmetric\n",
    "\n",
    "class EdgeConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, d, k, C, aggr=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if type(C) == int:\n",
    "            self.C = [C]*3\n",
    "        else:\n",
    "            self.C = C\n",
    "        \n",
    "        self.k = k\n",
    "\n",
    "        if aggr is None:\n",
    "            self.aggr = torch.mean\n",
    "        else:\n",
    "            self.aggr = aggr\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "\n",
    "        ### Shortcut path\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = d, out_channels = self.C[-1], kernel_size = 1, stride = 1),\n",
    "            nn.BatchNorm1d(self.C[-1])\n",
    "        )\n",
    "\n",
    "        ### Linear section, approximation of a MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(2*d, self.C[0], 1, 1),\n",
    "            nn.BatchNorm2d(self.C[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.C[0], self.C[1], 1, 1),\n",
    "            nn.BatchNorm2d(self.C[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.C[1], self.C[2], 1, 1),\n",
    "            nn.BatchNorm2d(self.C[2]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def kNN(self, x):\n",
    "        \"\"\"input: single jet data\n",
    "            output: tensor with shape [B, n, k, d] where d are the features of the knn particles\"\"\"\n",
    "        # expand the input tensor s.t. x_knn.shape = [B, n, n, d]\n",
    "        x_knn = x.unsqueeze(1).expand(-1, x.shape[1], -1, -1)\n",
    "\n",
    "        # calculate both delta_phi and delta_eta\n",
    "        delta_phieta = x_knn[:, :, :, :2] - x_knn[:, :, :, :2].transpose(1, 2)\n",
    "\n",
    "        # calculate distances and sort them in ascending order, keep only the indeces\n",
    "        _, indeces = torch.sqrt(torch.sum(delta_phieta**2, 3)**0.5).sort(dim=2, stable=True)\n",
    "\n",
    "        # keep the indeces of k nearest neighbours and use them to sort and cut the initial tensor\n",
    "        knn = indeces[:,:,:self.k]\n",
    "        x_knn = torch.gather(x_knn, 2, knn.unsqueeze(-1).expand(-1, -1, -1, x_knn.shape[-1]))\n",
    "\n",
    "        del delta_phieta, indeces, knn, _\n",
    "\n",
    "        return x_knn    # x_knn.shape = [B, n, k, d]\n",
    "\n",
    "    \n",
    "    def linear_aggregate(self, x):\n",
    "\n",
    "        # accepts as input [B, d, n, k]\n",
    "\n",
    "        # take the features of the particle and repeat them on the third axis\n",
    "        p_feat = x[:, :, :, 0].unsqueeze(3).expand(-1, -1, -1, self.k)\n",
    "\n",
    "        # now we can calculate knn features for each particle as a simple difference\n",
    "        knn_feat = x - p_feat\n",
    "\n",
    "        #print('p_feat:', p_feat.shape, '\\nknn_feat:', knn_feat.shape)\n",
    "\n",
    "        pairs = torch.concat([p_feat, knn_feat], dim=1)\n",
    "        del p_feat, knn_feat\n",
    "        print(pairs.shape) # expected [B, 2*d, n, k]\n",
    "\n",
    "        mlp_result = self.mlp(pairs)\n",
    "        del pairs\n",
    "\n",
    "        # aggregate\n",
    "        aggr_result = self.aggr(mlp_result, dim=3)\n",
    "\n",
    "        if type(aggr_result) is tuple:\n",
    "            aggr_result = aggr_result[0]\n",
    "        \n",
    "        return aggr_result\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size = [B, n, d]\n",
    "        # x_knn.size = [B, n, k, d]\n",
    "\n",
    "        x_knn = self.kNN(x).transpose(1, 3).transpose(2, 3)\n",
    "        shortcut = self.shortcut(x.transpose(1, 2))\n",
    "        x = self.linear_aggregate(x_knn)\n",
    "\n",
    "        x = self.act(x + shortcut).transpose(1, 2)\n",
    "        \n",
    "        del x_knn, shortcut\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ParticleNetDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_dim, n, d=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n = n\n",
    "\n",
    "        self.latent_to_up = nn.Sequential(\n",
    "            nn.Softmax(0),\n",
    "            nn.Linear(encoded_dim, 256),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 256)\n",
    "        )\n",
    "     \n",
    "        self.edgeconvs = nn.Sequential(\n",
    "            EdgeConv(256, 16, (256, 256, 128)),\n",
    "            EdgeConv(128, 16, (128, 128, 64)),\n",
    "            EdgeConv(64, 16, (64, 64, d))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        y = self.latent_to_up(x)\n",
    "\n",
    "        # up(dim)sampling through Conv2DTransposed on unsqueezed array\n",
    "        y = nn.ConvTranspose1d(1, self.n, 1, 1)(y.unsqueeze(1))\n",
    "        y = self.edgeconvs(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParticleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleNet(nn.Module):\n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # EDGE CONV PART\n",
    "        self.edge_conv = nn.Sequential(\n",
    "            EdgeConv(d=16, k=10, C=[64,64,64]),\n",
    "            EdgeConv(d=64, k=10, C=[128,128,128]),\n",
    "            EdgeConv(d=128, k=10, C=[256,256,256])) #output shape = [B,n,256]\n",
    "\n",
    "        self.final_part = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1))\n",
    "\n",
    "        # LATENT SPACE PROJECTION\n",
    "        # output size -> dimension of the latent space\n",
    "        self.latent_space = nn.Sequential(\n",
    "            nn.Linear(in_features = 256, out_features = encoded_space_dim),\n",
    "            nn.Softmax(0),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.edge_conv(x)\n",
    "        print(\"shape after edgeconv:\", y.shape)\n",
    "        y = nn.AvgPool1d(kernel_size=y.shape[1], stride=1)(y.transpose(1,2)).squeeze()\n",
    "        y = self.final_part(y)\n",
    "        y= self.latent_space(y)\n",
    "\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleDataset(Dataset):\n",
    "    # The ParticleDataset class inherits the Dataset class and implements the __init__, __len__, and __getitem__ methods\n",
    "\n",
    "    def __init__(self, path, transform=None):\n",
    "        # Initializing the ParticleDataset object.\n",
    "        # \"path\" is the path to the csv file containing the particle data.\n",
    "        # \"transform\" is an optional argument that specifies the transformations to be applied to the data.\n",
    "\n",
    "        # Read the csv file into a Pandas DataFrame.\n",
    "        self.x = pd.read_csv(path)\n",
    "\n",
    "        # Put the coordinates eta and phi as the first two features\n",
    "        self.x = self.x.reindex(\n",
    "            columns=[\"particlePolarPy\", \"particlePhi\"]\n",
    "            + [\n",
    "                col\n",
    "                for col in self.x.columns\n",
    "                if col not in [\"particlePolarPy\", \"particlePhi\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Store the \"transform\" argument.\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of particles in the dataset.\n",
    "        \"\"\"\n",
    "        # Return the number of rows in the DataFrame (i.e., the number of particles).\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the particles with jetID = idx.\n",
    "        \"\"\"\n",
    "        # Get the rows in the DataFrame that have a \"jetID\" column equal to \"idx\".\n",
    "        x = self.x[self.x.jetID==idx].to_numpy()\n",
    "        \n",
    "        # If \"transform\" was specified, apply it to the data.\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        # Return the transformed data.\n",
    "        return x\n",
    "    \n",
    "def custom_collate(batch):\n",
    "    \"\"\"\n",
    "    A custom collate function that can handle different shape tensors.\n",
    "    The default collate function provided by PyTorch's DataLoader assumes that all tensors in a batch have the same shape. \n",
    "    However, in our case, each \"datum\" is a set of particles that compose a jet and the number of particles composing a jet is not fixed. \n",
    "    Therefore, each tensor representing a jet has a different shape.\n",
    "\n",
    "    To handle this scenario, we need to override the collate function to be able to stack the tensors into a batch. \n",
    "    This function first determines the maximum number of particles among all jets in the batch. \n",
    "    Then, it pads all tensors with zeros to make sure they have the same shape. \n",
    "    Finally, it stacks the tensors along the batch dimension to return the padded data and original lengths.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the max number of particles among all the jets in the batch\n",
    "    n_part_max = max(x.shape[1] for x in batch)\n",
    "\n",
    "    # Pad all the tensors with zeros so they have the same shape\n",
    "    data = []\n",
    "    lengths = []\n",
    "    for x in batch:\n",
    "        n_part = x.shape[1]\n",
    "        data.append(torch.cat([x, torch.zeros(1, n_part_max - n_part, 16)], dim=1).squeeze())\n",
    "        lengths.append(n_part)\n",
    "\n",
    "    # Stack the tensors along the batch dimension\n",
    "    data = torch.stack(data)\n",
    "\n",
    "    # Return the padded data, original lengths, and target labels\n",
    "    return data, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventID</th>\n",
       "      <th>jetID</th>\n",
       "      <th>particleType</th>\n",
       "      <th>particleVx</th>\n",
       "      <th>particleVy</th>\n",
       "      <th>particleVz</th>\n",
       "      <th>particlePx</th>\n",
       "      <th>particlePy</th>\n",
       "      <th>particlePz</th>\n",
       "      <th>particleE</th>\n",
       "      <th>particlePolarPx</th>\n",
       "      <th>particlePolarPy</th>\n",
       "      <th>particlePolarPz</th>\n",
       "      <th>particlePolarE</th>\n",
       "      <th>particlePhi</th>\n",
       "      <th>particleTheta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-115.595071</td>\n",
       "      <td>5.513218</td>\n",
       "      <td>107.093643</td>\n",
       "      <td>157.675996</td>\n",
       "      <td>115.726471</td>\n",
       "      <td>0.827630</td>\n",
       "      <td>3.093935</td>\n",
       "      <td>2.347607e-01</td>\n",
       "      <td>3.093935</td>\n",
       "      <td>0.824122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-83.072377</td>\n",
       "      <td>4.831796</td>\n",
       "      <td>75.798599</td>\n",
       "      <td>112.561324</td>\n",
       "      <td>83.212776</td>\n",
       "      <td>0.816948</td>\n",
       "      <td>3.083494</td>\n",
       "      <td>5.078805e-01</td>\n",
       "      <td>3.083494</td>\n",
       "      <td>0.831991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-211</td>\n",
       "      <td>-0.981025</td>\n",
       "      <td>1.422285</td>\n",
       "      <td>-33.456345</td>\n",
       "      <td>-11.168506</td>\n",
       "      <td>-8.774579</td>\n",
       "      <td>9.043395</td>\n",
       "      <td>16.838385</td>\n",
       "      <td>14.203125</td>\n",
       "      <td>0.600055</td>\n",
       "      <td>-2.475661</td>\n",
       "      <td>1.395264e-01</td>\n",
       "      <td>-2.475661</td>\n",
       "      <td>1.003814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>0.073932</td>\n",
       "      <td>0.089866</td>\n",
       "      <td>-2.399344</td>\n",
       "      <td>-8.233158</td>\n",
       "      <td>-1.087632</td>\n",
       "      <td>6.647210</td>\n",
       "      <td>10.637351</td>\n",
       "      <td>8.304688</td>\n",
       "      <td>0.732994</td>\n",
       "      <td>-3.010249</td>\n",
       "      <td>-1.192093e-07</td>\n",
       "      <td>-3.010249</td>\n",
       "      <td>0.895801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-211</td>\n",
       "      <td>0.073905</td>\n",
       "      <td>0.089409</td>\n",
       "      <td>-2.399101</td>\n",
       "      <td>-8.048296</td>\n",
       "      <td>0.478376</td>\n",
       "      <td>6.097900</td>\n",
       "      <td>10.109785</td>\n",
       "      <td>8.062500</td>\n",
       "      <td>0.698202</td>\n",
       "      <td>3.082224</td>\n",
       "      <td>1.395264e-01</td>\n",
       "      <td>3.082224</td>\n",
       "      <td>0.923257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eventID  jetID  particleType  particleVx  particleVy  particleVz  \\\n",
       "0        0      0             0    0.000000    0.000000    0.000000   \n",
       "1        0      0             0    0.000000    0.000000    0.000000   \n",
       "2        0      0          -211   -0.981025    1.422285  -33.456345   \n",
       "3        0      0           130    0.073932    0.089866   -2.399344   \n",
       "4        0      0          -211    0.073905    0.089409   -2.399101   \n",
       "\n",
       "   particlePx  particlePy  particlePz   particleE  particlePolarPx  \\\n",
       "0 -115.595071    5.513218  107.093643  157.675996       115.726471   \n",
       "1  -83.072377    4.831796   75.798599  112.561324        83.212776   \n",
       "2  -11.168506   -8.774579    9.043395   16.838385        14.203125   \n",
       "3   -8.233158   -1.087632    6.647210   10.637351         8.304688   \n",
       "4   -8.048296    0.478376    6.097900   10.109785         8.062500   \n",
       "\n",
       "   particlePolarPy  particlePolarPz  particlePolarE  particlePhi  \\\n",
       "0         0.827630         3.093935    2.347607e-01     3.093935   \n",
       "1         0.816948         3.083494    5.078805e-01     3.083494   \n",
       "2         0.600055        -2.475661    1.395264e-01    -2.475661   \n",
       "3         0.732994        -3.010249   -1.192093e-07    -3.010249   \n",
       "4         0.698202         3.082224    1.395264e-01     3.082224   \n",
       "\n",
       "   particleTheta  \n",
       "0       0.824122  \n",
       "1       0.831991  \n",
       "2       1.003814  \n",
       "3       0.895801  \n",
       "4       0.923257  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particle_df_path = '../data/particle_df.csv'\n",
    "particle_preproc_df_path = '../data/particle_df_preprocessed.csv'\n",
    "\n",
    "par_pre_df = pd.read_csv(particle_preproc_df_path)\n",
    "par_df = pd.read_csv(particle_df_path)\n",
    "par_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 41, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 62, 16])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Compose object that applies the \"ToTensor\" transformation.\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create a ParticleDataset object using the csv file located at \"particle_df_path\" and the \"train_transform\" transformations.\n",
    "train_data = ParticleDataset(particle_df_path, train_transform)\n",
    "\n",
    "# Access the first element in the dataset to get its shape.\n",
    "print(train_data[1].shape)\n",
    "\n",
    "batch_size       = 10\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, collate_fn=custom_collate)\n",
    "test = train_dataloader._get_iterator()._next_data()[0]\n",
    "test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ParticleNet(4)\n",
    "\n",
    "decoder = ParticleNetDecoder(4, test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 62, 10])\n",
      "torch.Size([10, 128, 62, 10])\n",
      "torch.Size([10, 256, 62, 10])\n",
      "shape after edgeconv: torch.Size([10, 62, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = encoder.forward(test.float())\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 62, 16])\n",
      "torch.Size([10, 256, 62, 16])\n",
      "torch.Size([10, 128, 62, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 62, 16])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = decoder.forward(encoded)\n",
    "decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "331a419315a2fdb77d716bff9d27d64bbb9fafc97272ccf1eb9acf778ff8f493"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
