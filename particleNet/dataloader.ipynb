{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_df_path = '../data/particle_df.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleDataset(Dataset):\n",
    "    # The ParticleDataset class inherits the Dataset class and implements the __init__, __len__, and __getitem__ methods\n",
    "\n",
    "    def __init__(self, path, transform=None):\n",
    "        # Initializing the ParticleDataset object.\n",
    "        # \"path\" is the path to the csv file containing the particle data.\n",
    "        # \"transform\" is an optional argument that specifies the transformations to be applied to the data.\n",
    "        \n",
    "        # Read the csv file into a Pandas DataFrame.\n",
    "        self.x = pd.read_csv(path)\n",
    "        \n",
    "        # Store the \"transform\" argument.\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of particles in the dataset.\n",
    "        \"\"\"\n",
    "        # Return the number of rows in the DataFrame (i.e., the number of particles).\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the particles with jetID = idx.\n",
    "        \"\"\"\n",
    "        # Get the rows in the DataFrame that have a \"jetID\" column equal to \"idx\".\n",
    "        x = self.x[self.x.jetID==idx].to_numpy()\n",
    "        \n",
    "        # If \"transform\" was specified, apply it to the data.\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        # Return the transformed data.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Compose object that applies the \"ToTensor\" transformation.\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Create a ParticleDataset object using the csv file located at \"particle_df_path\" and the \"train_transform\" transformations.\n",
    "train_data = ParticleDataset(particle_df_path, train_transform)\n",
    "\n",
    "# Access the first element in the dataset to get its shape.\n",
    "train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    \"\"\"\n",
    "    A custom collate function that can handle different shape tensors.\n",
    "    The default collate function provided by PyTorch's DataLoader assumes that all tensors in a batch have the same shape. \n",
    "    However, in our case, each \"datum\" is a set of particles that compose a jet and the number of particles composing a jet is not fixed. \n",
    "    Therefore, each tensor representing a jet has a different shape.\n",
    "\n",
    "    To handle this scenario, we need to override the collate function to be able to stack the tensors into a batch. \n",
    "    This function first determines the maximum number of particles among all jets in the batch. \n",
    "    Then, it pads all tensors with zeros to make sure they have the same shape. \n",
    "    Finally, it stacks the tensors along the batch dimension to return the padded data and original lengths.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the max number of particles among all the jets in the batch\n",
    "    n_part_max = max(x.shape[1] for x in batch)\n",
    "\n",
    "    # Pad all the tensors with zeros so they have the same shape\n",
    "    data = []\n",
    "    lengths = []\n",
    "    for x in batch:\n",
    "        n_part = x.shape[1]\n",
    "        data.append(torch.cat([x, torch.zeros(1, n_part_max - n_part, 16)], dim=1))\n",
    "        lengths.append(n_part)\n",
    "\n",
    "    # Stack the tensors along the batch dimension\n",
    "    data = torch.stack(data)\n",
    "\n",
    "    # Return the padded data, original lengths, and target labels\n",
    "    return data, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size       = 10\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 62, 16]) [23, 41, 26, 20, 62, 35, 9, 4, 46, 48]\n",
      "torch.Size([10, 1, 66, 16]) [41, 22, 2, 40, 21, 26, 25, 66, 28, 2]\n"
     ]
    }
   ],
   "source": [
    "# loop over the dataloader to get the data in batches\n",
    "i=0\n",
    "for batch, original_length in train_dataloader:\n",
    "    print(batch.shape, original_length)\n",
    "    i+=1\n",
    "    if i==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward pass of your model, you can use the original lengths to process the data correctly, for example, by masking out the padded zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing nested tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = []\n",
    "\n",
    "for i, original_shape in enumerate(original_length):\n",
    "    # Slice the tensor along the third dimension to get the desired shape\n",
    "    a = batch[i, :, :original_shape, :]\n",
    "    tensors.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 41, 16])\n",
      "torch.Size([1, 22, 16])\n",
      "torch.Size([1, 2, 16])\n",
      "torch.Size([1, 40, 16])\n",
      "torch.Size([1, 21, 16])\n",
      "torch.Size([1, 26, 16])\n",
      "torch.Size([1, 25, 16])\n",
      "torch.Size([1, 66, 16])\n",
      "torch.Size([1, 28, 16])\n",
      "torch.Size([1, 2, 16])\n"
     ]
    }
   ],
   "source": [
    "for t in tensors:\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'nested_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nested \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnested_tensor(tensors, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'nested_tensor'"
     ]
    }
   ],
   "source": [
    "nested = torch.nested_tensor(tensors, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 273, 16])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e66dcae97b701b915eadc4438c910e988ef9c71a5edf15724345fc137d8a1821"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
