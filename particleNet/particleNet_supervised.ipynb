{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm # this module is useful to plot progress bars\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from DataReader import DataReader\n",
    "\n",
    "from particlenet import ParticleNet\n",
    "from particlenet_decoder import ParticleNetDecoder, ParticleNetDecoder_1feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_reader, n_files=None, transform=None):\n",
    "\n",
    "        data_reader.read_files(n_files=n_files)\n",
    "\n",
    "        self.x = data_reader.get_features()\n",
    "        self.y = data_reader.get_labels()\n",
    "\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file jetImage_5_100p_20000_30000.h5\n",
      "Reading file jetImage_2_100p_60000_70000.h5\n",
      "Reading file jetImage_3_100p_20000_30000.h5\n",
      "Reading file jetImage_6_100p_50000_60000.h5\n",
      "Reading file jetImage_4_100p_20000_30000.h5\n",
      "Reading file jetImage_0_100p_80000_90000.h5\n",
      "Reading file jetImage_5_100p_10000_20000.h5\n",
      "Reading file jetImage_3_100p_40000_50000.h5\n",
      "Reading file jetImage_2_100p_50000_60000.h5\n",
      "Reading file jetImage_2_100p_10000_20000.h5\n"
     ]
    }
   ],
   "source": [
    "train_transform = None #transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "data_reader = DataReader(\"../data/train/\")\n",
    "\n",
    "train_data = ParticleDataset(data_reader=data_reader, n_files=10, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 100, 7]), torch.float32, torch.Size([256, 6]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size       = 256\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "next(iter(train_dataloader))[0].shape, next(iter(train_dataloader))[0].dtype, next(iter(train_dataloader))[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_dim = 2\n",
    "n_features = 7\n",
    "\n",
    "particlenet = ParticleNet(n_features, latent_space_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParticleNet(\n",
       "  (edge_conv): Sequential(\n",
       "    (0): EdgeConv(\n",
       "      (act): ReLU()\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv1d(7, 64, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Conv2d(14, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1): EdgeConv(\n",
       "      (act): ReLU()\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): EdgeConv(\n",
       "      (act): ReLU()\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_part): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (latent_space): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (1): Softmax(dim=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Define the loss function\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "params_to_optimize = [\n",
    "    {'params': particlenet.parameters()}\n",
    "]\n",
    "\n",
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "particlenet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LinearLR, ExponentialLR\n",
    "\n",
    "### Define an optimizer (both for the encoder and the decoder!)\n",
    "lr = 3e-4 # Learning rate\n",
    "\n",
    "#function for the custom learning rate:\n",
    "def custom_lr(epoch, lr = 3e-4):\n",
    "\n",
    "    if epoch < 8: # linear increase\n",
    "        return lr + 1.125e-4 * epoch\n",
    "    if (epoch >= 8 and epoch < 16): # linear decrease\n",
    "        return lr*10 - 1.125e-4 * (epoch - 8 )\n",
    "    if epoch >= 16: # linear decrease\n",
    "        return lr - 7.4825e-5 * (epoch - 16)\n",
    "\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=custom_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training function\n",
    "def train_epoch(encoder, device, dataloader, loss_fn, optimizer):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    encoder.train()\n",
    "\n",
    "    # abbiamo gi√† definito l'optimizer nella cella precedente\n",
    "    \n",
    "    losses = []\n",
    "    accus  = []\n",
    "    pbar = tqdm(dataloader, 'Steps')\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for batch, label in pbar: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        \n",
    "        batch = batch.to(device)\n",
    "        label = label[:,-2].float().to(device)\n",
    "        \n",
    "        #encode\n",
    "        y_encoder_pred = encoder(batch)[:,-2]\n",
    "\n",
    "        loss = loss_fn(y_encoder_pred, label) # funziona anche per le matrici in teoria\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        accus.append((1-torch.mean(torch.abs(torch.round(y_encoder_pred)-label))).detach().cpu().numpy())\n",
    "        pbar.set_postfix_str(f'loss: {losses[-1]:.3f}, acc: {accus[-1]:.4f}')\n",
    "    \n",
    "    losses = np.mean(losses)\n",
    "    accus  = np.mean(accus)\n",
    "    pbar.set_postfix_str(f'mean loss: {losses:.2f}, mean accuracy: {accus:.4f}', refresh = True)\n",
    "    return losses, accus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing function\n",
    "def test_epoch(encoder, device, dataloader, loss_fn):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    encoder.eval()\n",
    "\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        for image_batch, label in tqdm(dataloader, 'Steps'):\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.to(device)\n",
    "            # Encode data\n",
    "            encoded_data = encoder(image_batch)\n",
    "            # Append the network output and the original image to the lists\n",
    "            conc_out.append(encoded_data.cpu())\n",
    "            conc_label.append(torch.stack([label[:,-2], 1-label[:,-2]], dim=1).float().cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    return val_loss.data, 1-torch.mean(torch.abs(conc_label-conc_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdec9cc5aa9438d82d9f03adaa94be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6158fb9da6a48c594c8b5b6b15a7b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d66ff27b304f20b82ce60d07f7a09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0943ba79da445ab37f3bdcd34de3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e92ff526ae346da8e0a5a98f737179a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2063c725aa4a16b46cf419c3cc130c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4236e81d2aa4eb1a60ad343607e3655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dae8d8763ae47d7bd4fa6ccde21bb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44d1833cbaf4590ad0df32c3712d91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c00f7cfd204a9b8500fe8c5279403e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a12e7c78564ac5ba9b1057dcf28b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c9ef9e2a9448479c11440f95aeeda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c181dffe8140f791060b50115ed7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3362473ccafc487e98cd84a2b3f088c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd210a9577b74e93ad2546bc243edab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2104e480c726470bbc0c43e97b1d1dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2a85124e57435995f4ddf543c5f46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4405ce7eea647139ad7a5a350c6df91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecd85a56115412d9c5cad0a6146f8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5a1de9a49d42678a33188616c7abed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf8a7b77b604d5d9b8c54fb55841807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Training cycle\n",
    "num_epochs = 20\n",
    "pbar = tqdm(range(num_epochs), 'Epoch')\n",
    "for epoch in pbar:\n",
    "\n",
    "    ### Training (use the training function)\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        encoder=particlenet,\n",
    "        device=device, \n",
    "        dataloader=train_dataloader, \n",
    "        loss_fn=loss_fn, \n",
    "        optimizer=optim)\n",
    "    pbar.set_postfix_str(f'loss: {train_loss:.2f}, acc: {train_acc:.4f}')\n",
    "    '''\n",
    "    ### Validation  (use the testing function)\n",
    "    val_loss = test_epoch(\n",
    "        encoder=encoder, \n",
    "        decoder=decoder, \n",
    "        device=device, \n",
    "        dataloader=test_dataloader, \n",
    "        loss_fn=loss_fn)\n",
    "    # Print Validationloss\n",
    "    print(f'VALIDATION - EPOCH {epoch+1}/{num_epochs} - loss: {val_loss}\\n')\n",
    "    '''\n",
    "    ### step of the lr scheduler\n",
    "    scheduler.step() # alla fine di ogni epoca\n",
    "\n",
    "    ### Plot progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470a0e78afee41f3af7ba9e63ccdcc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top: tensor(20378)\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(train_dataloader, 'Steps')\n",
    "count = 0\n",
    "# Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "for batch, label in pbar: \n",
    "    count = count + torch.sum(label[:, -2])\n",
    "\n",
    "print('top:', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2314458721ceb093a16fc62a007e4288d5fdb68183e3d86ed8ecd92512e98f9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
